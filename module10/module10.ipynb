{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 10: Introduction to Hugging Face Transformers\n",
    "\n",
    "This notebook introduces Hugging Face's ecosystem and demonstrates how to:\n",
    "1. Set up the environment for local models\n",
    "2. Load your first model\n",
    "3. Perform basic inference\n",
    "4. Compare local models with API-based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries\n",
    "\n",
    "First, let's install the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Uncomment and run this cell if you need to install the libraries\n",
    "# !pip install torch transformers accelerate gradio python-dotenv\n",
    "# !pip install bitsandbytes  # Optional: for 4-bit quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to Hugging Face's Ecosystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def introduction_to_huggingface():\n",
    "    \"\"\"Print an introduction to Hugging Face's ecosystem\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"INTRODUCTION TO HUGGING FACE TRANSFORMERS\".center(80))\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\"\"\n",
    "Hugging Face is an AI community and platform that provides:\n",
    "\n",
    "1. ğŸ¤— Model Hub: A repository of pre-trained models (100,000+) for NLP, computer vision, \n",
    "   audio processing, and more.\n",
    "\n",
    "2. ğŸ”§ Transformers Library: A Python library that provides APIs and tools to easily \n",
    "   download and train state-of-the-art pretrained models.\n",
    "\n",
    "3. ğŸ“š Datasets: A library and platform for easily sharing and accessing datasets.\n",
    "\n",
    "4. ğŸ§ª Spaces: A platform for hosting ML demo apps.\n",
    "\n",
    "5. ğŸ§  AutoTrain: Tools for training models without writing code.\n",
    "\n",
    "Key advantages of using Hugging Face for local models:\n",
    "- Run models on your own hardware without API costs\n",
    "- Full control over model parameters and behavior\n",
    "- Privacy - data doesn't leave your machine\n",
    "- Ability to fine-tune models for specific use cases\n",
    "- No internet connection required for inference\n",
    "    \"\"\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "introduction_to_huggingface()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check if CUDA is available\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Print PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Print GPU info if available\n",
    "if DEVICE == \"cuda\":\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loading a Model\n",
    "\n",
    "Let's load a small model first. We'll use TinyLlama, which is a 1.1B parameter model that can run on most hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_model(model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", use_4bit=True):\n",
    "    \"\"\"Load a model from Hugging Face Hub\"\"\"\n",
    "    print(f\"\\nLoading model: {model_name}\")\n",
    "    print(\"This may take a few moments depending on your internet connection and the model size...\")\n",
    "    \n",
    "    try:\n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Load model with quantization if requested\n",
    "        if use_4bit and DEVICE == \"cuda\":\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.float16,\n",
    "                load_in_4bit=True\n",
    "            )\n",
    "            print(\"Model loaded with 4-bit quantization\")\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
    "                torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
    "            )\n",
    "            print(f\"Model loaded in {'16-bit' if DEVICE == 'cuda' else '32-bit'} precision\")\n",
    "        \n",
    "        return model, tokenizer\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        print(\"\\nTroubleshooting tips:\")\n",
    "        print(\"1. Check your internet connection\")\n",
    "        print(\"2. Verify the model name is correct\")\n",
    "        print(\"3. Try a smaller model if you're running out of memory\")\n",
    "        print(\"4. Make sure you have the latest transformers library\")\n",
    "        return None, None\n",
    "\n",
    "# Load a small model\n",
    "model, tokenizer = load_model(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", use_4bit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Basic Inference\n",
    "\n",
    "Now let's perform basic inference with our loaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def basic_inference(model, tokenizer, prompt, max_length=512, temperature=0.7):\n",
    "    \"\"\"Perform basic inference with a loaded model\"\"\"\n",
    "    if model is None or tokenizer is None:\n",
    "        return \"Model or tokenizer not loaded correctly.\"\n",
    "    \n",
    "    try:\n",
    "        # Format the prompt based on model type\n",
    "        if \"llama\" in model.config.architectures[0].lower():\n",
    "            # Format for Llama models\n",
    "            formatted_prompt = f\"<|user|>\\n{prompt}\\n<|assistant|>\\n\"\n",
    "        elif \"mistral\" in model.config.architectures[0].lower():\n",
    "            # Format for Mistral models\n",
    "            formatted_prompt = f\"[INST] {prompt} [/INST]\"\n",
    "        elif \"phi\" in model.config.architectures[0].lower():\n",
    "            # Format for Phi models\n",
    "            formatted_prompt = f\"User: {prompt}\\nAssistant:\"\n",
    "        else:\n",
    "            # Default format\n",
    "            formatted_prompt = prompt\n",
    "        \n",
    "        # Tokenize the prompt\n",
    "        inputs = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        # Move inputs to the appropriate device\n",
    "        if DEVICE == \"cuda\":\n",
    "            inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate text\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_length=max_length,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode the generated text\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract only the assistant's response\n",
    "        if \"<|assistant|>\" in formatted_prompt:\n",
    "            response = generated_text.split(\"<|assistant|>\")[-1].strip()\n",
    "        elif \"[/INST]\" in formatted_prompt:\n",
    "            response = generated_text.split(\"[/INST]\")[-1].strip()\n",
    "        elif \"Assistant:\" in generated_text:\n",
    "            response = generated_text.split(\"Assistant:\")[-1].strip()\n",
    "        else:\n",
    "            response = generated_text.replace(prompt, \"\").strip()\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error during inference: {str(e)}\"\n",
    "\n",
    "# Test the model with a few prompts\n",
    "test_prompts = [\n",
    "    \"What are the main features of Python?\",\n",
    "    \"Write a short poem about artificial intelligence.\",\n",
    "    \"Explain quantum computing to a 10-year-old.\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f\"\\n\\nPrompt {i+1}: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "    response = basic_inference(model, tokenizer, prompt)\n",
    "    print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experimenting with Generation Parameters\n",
    "\n",
    "Let's see how different parameters affect the generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test with different temperatures\n",
    "prompt = \"Write a creative story about a robot who discovers emotions.\"\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "temperatures = [0.3, 0.7, 1.2]\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nTemperature: {temp}\")\n",
    "    print(\"-\" * 50)\n",
    "    response = basic_inference(model, tokenizer, prompt, temperature=temp)\n",
    "    print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparing Local Models vs. API-Based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def compare_local_vs_api():\n",
    "    \"\"\"Print a comparison between local models and API-based models\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"LOCAL MODELS VS. API-BASED MODELS\".center(80))\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     â”‚ Local Models             â”‚ API-Based Models        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Cost                â”‚ One-time hardware cost   â”‚ Pay per token/request   â”‚\n",
    "â”‚ Privacy             â”‚ Data stays on device     â”‚ Data sent to servers    â”‚\n",
    "â”‚ Setup Complexity    â”‚ Higher                   â”‚ Lower                   â”‚\n",
    "â”‚ Maintenance         â”‚ Manual updates needed    â”‚ Automatic updates       â”‚\n",
    "â”‚ Performance         â”‚ Depends on hardware      â”‚ Consistent              â”‚\n",
    "â”‚ Customization       â”‚ Full control             â”‚ Limited by API          â”‚\n",
    "â”‚ Scaling             â”‚ Limited by hardware      â”‚ Easy to scale           â”‚\n",
    "â”‚ Offline Usage       â”‚ Yes                      â”‚ No                      â”‚\n",
    "â”‚ Model Size Options  â”‚ Limited by hardware      â”‚ Wide range available    â”‚\n",
    "â”‚ Latency             â”‚ Lower (no network)       â”‚ Higher (network delay)  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "When to use local models:\n",
    "- Privacy-sensitive applications\n",
    "- Offline environments\n",
    "- Cost-sensitive long-running applications\n",
    "- When you need full control over the model\n",
    "\n",
    "When to use API-based models:\n",
    "- Quick prototyping\n",
    "- Limited local hardware\n",
    "- Need for state-of-the-art large models\n",
    "- Simplicity is prioritized over customization\n",
    "    \"\"\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "compare_local_vs_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Creating a Simple Gradio Interface (Optional)\n",
    "\n",
    "If you want to create a user interface for your model, you can use Gradio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import gradio as gr\n",
    "\n",
    "def create_gradio_interface():\n",
    "    \"\"\"Create a simple Gradio interface for the model\"\"\"\n",
    "    def generate(prompt, temperature=0.7, max_length=512):\n",
    "        return basic_inference(model, tokenizer, prompt, max_length=max_length, temperature=temperature)\n",
    "    \n",
    "    demo = gr.Interface(\n",
    "        fn=generate,\n",
    "        inputs=[\n",
    "            gr.Textbox(lines=4, placeholder=\"Enter your prompt here...\", label=\"Prompt\"),\n",
    "            gr.Slider(0.1, 1.5, value=0.7, label=\"Temperature\"),\n",
    "            gr.Slider(64, 1024, value=512, step=64, label=\"Max Length\")\n",
    "        ],\n",
    "        outputs=gr.Textbox(label=\"Generated Text\"),\n",
    "        title=\"Local LLM Demo\",\n",
    "        description=\"Generate text using a local language model\"\n",
    "    )\n",
    "    return demo\n",
    "\n",
    "# Uncomment to create and launch the interface\n",
    "# interface = create_gradio_interface()\n",
    "# interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this notebook, we've explored:\n",
    "\n",
    "1. Hugging Face's ecosystem and its components\n",
    "2. How to set up the environment for local models\n",
    "3. Loading a model from Hugging Face Hub\n",
    "4. Performing basic inference with the model\n",
    "5. Experimenting with different generation parameters\n",
    "6. Comparing local models with API-based models\n",
    "\n",
    "Next steps:\n",
    "- Try different models from the Hugging Face Hub\n",
    "- Experiment with fine-tuning models on your own data\n",
    "- Explore more advanced inference parameters\n",
    "- Integrate local models into your applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
