"""
Explainable AI Demo for VyaparBazaar Analytics.

This script demonstrates how to use the ML features generated by the dbt models
to build explainable AI models for:
1. Customer Churn Prediction
2. Customer Segmentation
3. Product Recommendations

It uses SHAP values to explain model predictions and visualize feature importance.
"""

import os
import duckdb
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, silhouette_score
import shap

# Set up paths
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DUCKDB_PATH = os.path.join(BASE_DIR, 'data', 'vyaparbazaar.duckdb')
OUTPUT_DIR = os.path.join(BASE_DIR, 'data', 'explainable_ai_output')

# Create output directory if it doesn't exist
os.makedirs(OUTPUT_DIR, exist_ok=True)

def load_data_from_duckdb():
    """Load the ML feature data from DuckDB"""
    print("Loading data from DuckDB...")

    # Connect to DuckDB
    con = duckdb.connect(DUCKDB_PATH)

    # Load the ML feature tables
    try:
        # Try with schema prefix first
        churn_features = con.execute("SELECT * FROM main_ml_features.customer_churn_features_v2").fetchdf()
        print("Using enhanced churn features model (v2) with additional behavioral and temporal features")
    except Exception as e:
        # Fall back to regular churn features if v2 doesn't exist
        print(f"Warning: {e}")
        print("Falling back to regular customer_churn_features")
        churn_features = con.execute("SELECT * FROM main_ml_features.customer_churn_features").fetchdf()

    segmentation_features = con.execute("SELECT * FROM main_ml_features.customer_segmentation_features").fetchdf()
    recommendation_features = con.execute("SELECT * FROM main_ml_features.product_recommendation_features").fetchdf()

    # Close connection
    con.close()

    return churn_features, segmentation_features, recommendation_features

def preprocess_churn_data(df):
    """Preprocess the churn data for modeling"""
    print("Preprocessing churn data...")

    # Select relevant features for v2 model
    features = [
        'days_since_last_order', 'order_count', 'total_order_value',
        'avg_order_value', 'order_frequency_last_90_days', 'order_frequency_last_30_days',
        'order_value_trend', 'days_between_first_second_order', 'product_category_diversity',
        'has_returned_item', 'refund_rate', 'weekend_shopper_ratio', 'evening_shopper_ratio'
    ]

    # Filter to only include columns that exist in the dataframe
    features = [f for f in features if f in df.columns]

    # Select features and target
    X = df[features]
    y = df['is_churned']

    # Handle missing values
    X = X.fillna(0)

    return X, y

def build_churn_model(X, y):
    """Build and explain a churn prediction model"""
    print("Building churn prediction model...")

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Train model
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)

    # Evaluate model
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Churn Model Accuracy: {accuracy:.4f}")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

    # Generate SHAP values for explainability
    print("Generating SHAP values for model explanation...")
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X_test)

    # Check if we have a binary classification problem with two classes in the output
    if isinstance(shap_values, list) and len(shap_values) > 1:
        # Binary classification with two classes in output
        shap_values_for_plot = shap_values[1]  # Use the positive class
    else:
        # Single class or regression problem
        shap_values_for_plot = shap_values

    # Plot feature importance
    plt.figure(figsize=(10, 6))
    shap.summary_plot(shap_values_for_plot, X_test, plot_type="bar", show=False)
    plt.title("Feature Importance for Churn Prediction")
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, 'churn_feature_importance.png'))

    # Plot SHAP values
    plt.figure(figsize=(12, 8))
    shap.summary_plot(shap_values_for_plot, X_test, show=False)
    plt.title("SHAP Values for Churn Prediction")
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, 'churn_shap_values.png'))

    # Generate explanations for a few examples
    print("\nExample Churn Predictions with Explanations:")
    for i in range(min(5, len(X_test))):
        # Get prediction probabilities
        proba = model.predict_proba(X_test.iloc[i:i+1])[0]

        # Handle the case where we only have one class
        if len(proba) == 1:
            prediction = proba[0]
            print(f"\nCustomer {i+1}:")
            print(f"Churn Probability: {prediction:.2f} (Note: Only one class detected)")
        else:
            prediction = proba[1]  # Probability of class 1 (churn)
            print(f"\nCustomer {i+1}:")
            print(f"Churn Probability: {prediction:.2f}")

        # Get top contributing features
        instance_shap_values = explainer.shap_values(X_test.iloc[i:i+1])

        # Handle different SHAP value formats
        if isinstance(instance_shap_values, list) and len(instance_shap_values) > 1:
            # Binary classification with two classes in output
            instance_values = instance_shap_values[1][0]
        else:
            # Single class or regression problem
            instance_values = instance_shap_values[0]

        feature_names = X_test.columns
        feature_importance = list(zip(feature_names, instance_values))
        feature_importance.sort(key=lambda x: abs(x[1]), reverse=True)

        print("Top factors influencing this prediction:")
        for feature, importance in feature_importance[:5]:
            direction = "increasing" if importance > 0 else "decreasing"
            print(f"- {feature}: {abs(importance):.4f} ({direction} churn risk)")

    return model, explainer

def preprocess_segmentation_data(df):
    """Preprocess the segmentation data for clustering"""
    print("Preprocessing segmentation data...")

    # Select relevant features
    features = [
        'recency_days', 'frequency', 'monetary_value',
        'order_count', 'total_order_value', 'avg_order_value',
        'days_since_last_order', 'total_events', 'total_app_events',
        'cart_to_order_conversion_rate', 'avg_review_score', 'total_tickets'
    ]

    # Select features
    X = df[features].copy()

    # Handle missing values
    X = X.fillna(0)

    # Scale the data
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    return X, X_scaled, features

def build_segmentation_model(X, X_scaled, features):
    """Build and explain a customer segmentation model"""
    print("Building customer segmentation model...")

    # Determine optimal number of clusters
    silhouette_scores = []
    for k in range(2, 11):
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(X_scaled)
        silhouette_scores.append(silhouette_score(X_scaled, cluster_labels))

    # Choose optimal k
    optimal_k = np.argmax(silhouette_scores) + 2
    print(f"Optimal number of clusters: {optimal_k}")

    # Build final model
    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(X_scaled)

    # Add cluster labels to original data
    X_with_clusters = X.copy()
    X_with_clusters['cluster'] = cluster_labels

    # Analyze clusters
    cluster_analysis = X_with_clusters.groupby('cluster').mean()
    print("\nCluster Analysis:")
    print(cluster_analysis)

    # Plot cluster characteristics
    plt.figure(figsize=(15, 10))
    for i, feature in enumerate(features):
        plt.subplot(4, 3, i+1)
        sns.boxplot(x='cluster', y=feature, data=X_with_clusters)
        plt.title(feature)
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, 'customer_segments_characteristics.png'))

    # Create segment descriptions
    segment_descriptions = []
    for cluster_id in range(optimal_k):
        cluster_data = cluster_analysis.loc[cluster_id]

        # Determine key characteristics
        if cluster_data['recency_days'] < 30 and cluster_data['frequency'] > 3:
            loyalty = "Loyal"
        elif cluster_data['recency_days'] < 90:
            loyalty = "Active"
        else:
            loyalty = "Inactive"

        if cluster_data['monetary_value'] > cluster_analysis['monetary_value'].mean() * 1.5:
            value = "High-value"
        elif cluster_data['monetary_value'] > cluster_analysis['monetary_value'].mean() * 0.5:
            value = "Medium-value"
        else:
            value = "Low-value"

        if cluster_data['total_events'] > cluster_analysis['total_events'].mean() * 1.5:
            engagement = "Highly engaged"
        elif cluster_data['total_events'] > cluster_analysis['total_events'].mean() * 0.5:
            engagement = "Moderately engaged"
        else:
            engagement = "Low engagement"

        # Create segment name and description
        segment_name = f"Segment {cluster_id}: {loyalty} {value} {engagement} Customers"

        description = f"""
        {segment_name}

        Key Characteristics:
        - Average Order Value: â‚¹{cluster_data['avg_order_value']:.2f}
        - Order Frequency: {cluster_data['frequency']:.1f} orders
        - Days Since Last Order: {cluster_data['recency_days']:.1f} days
        - Total Engagement Events: {cluster_data['total_events']:.1f}
        - App Engagement: {cluster_data['total_app_events']:.1f} events
        - Average Review Score: {cluster_data['avg_review_score']:.1f}/5

        Marketing Recommendations:
        """

        # Add segment-specific recommendations
        if loyalty == "Loyal" and value == "High-value":
            description += "- Focus on retention and premium offerings\n"
            description += "- Exclusive loyalty rewards and early access to new products\n"
            description += "- Personalized communication and VIP customer service"
        elif loyalty == "Loyal" and value != "High-value":
            description += "- Upselling opportunities to increase order value\n"
            description += "- Loyalty rewards to maintain engagement\n"
            description += "- Targeted promotions for complementary products"
        elif loyalty == "Active" and value == "High-value":
            description += "- Increase purchase frequency with targeted offers\n"
            description += "- Premium product recommendations\n"
            description += "- Encourage loyalty program enrollment"
        elif loyalty == "Active" and value != "High-value":
            description += "- Focus on increasing both frequency and order value\n"
            description += "- Category-specific promotions\n"
            description += "- Encourage app usage for more engagement"
        elif loyalty == "Inactive" and value == "High-value":
            description += "- Win-back campaigns with personalized offers\n"
            description += "- Reminder of previous positive experiences\n"
            description += "- New product announcements aligned with past purchases"
        else:
            description += "- Re-engagement campaigns with special offers\n"
            description += "- Simplified shopping experience\n"
            description += "- Value-oriented messaging"

        segment_descriptions.append(description)

    # Print segment descriptions
    print("\nCustomer Segment Descriptions:")
    for description in segment_descriptions:
        print(description)
        print("-" * 80)

    # Save segment descriptions to file
    with open(os.path.join(OUTPUT_DIR, 'customer_segments.txt'), 'w', encoding='utf-8') as f:
        for description in segment_descriptions:
            f.write(description + "\n" + "-" * 80 + "\n")

    return kmeans, cluster_labels, segment_descriptions

def main():
    """Main function to run the explainable AI demo"""
    print("Starting Explainable AI Demo for VyaparBazaar Analytics...")

    # Load data
    churn_features, segmentation_features, recommendation_features = load_data_from_duckdb()

    # Check if data was loaded successfully
    if churn_features.empty or segmentation_features.empty or recommendation_features.empty:
        print("Error: Could not load data from DuckDB. Please make sure the pipeline has been run.")
        return

    print(f"Loaded {len(churn_features)} customer records for churn analysis")
    print(f"Loaded {len(segmentation_features)} customer records for segmentation")
    print(f"Loaded {len(recommendation_features)} product-customer interactions for recommendations")

    # Churn Prediction
    print("\n" + "="*80)
    print("EXPLAINABLE CHURN PREDICTION")
    print("="*80)
    X_churn, y_churn = preprocess_churn_data(churn_features)
    churn_model, churn_explainer = build_churn_model(X_churn, y_churn)

    # Customer Segmentation
    print("\n" + "="*80)
    print("EXPLAINABLE CUSTOMER SEGMENTATION")
    print("="*80)
    X_seg, X_seg_scaled, seg_features = preprocess_segmentation_data(segmentation_features)
    seg_model, cluster_labels, segment_descriptions = build_segmentation_model(X_seg, X_seg_scaled, seg_features)

    print("\n" + "="*80)
    print("EXPLAINABLE AI DEMO COMPLETED")
    print("="*80)
    print(f"Output files saved to: {OUTPUT_DIR}")

if __name__ == "__main__":
    main()
